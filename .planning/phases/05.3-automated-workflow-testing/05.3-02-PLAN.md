---
phase: 05.3-automated-workflow-testing
plan: 02
type: execute
wave: 2
depends_on: ["05.3-01"]
files_modified:
  - src/executor/test-harness.ts
  - src/executor/types.ts
autonomous: true

must_haves:
  truths:
    - "testWorkflow() deploys, executes, asserts, and cleans up in one call"
    - "Test scenarios define input payloads paired with expected outcomes"
    - "On failure, actual output and errors are returned in a format Claude can diagnose"
  artifacts:
    - path: "src/executor/test-harness.ts"
      provides: "testWorkflow function — deploy/execute/assert/cleanup loop"
      exports: ["testWorkflow"]
    - path: "src/executor/types.ts"
      provides: "TestScenario, TestResult, TestReport types added"
      exports: ["TestScenario", "TestResult", "TestReport"]
  key_links:
    - from: "src/executor/test-harness.ts"
      to: "src/deployer/deploy.ts"
      via: "deployWorkflow() to create workflow in n8n"
      pattern: "deployWorkflow"
    - from: "src/executor/test-harness.ts"
      to: "src/executor/execute.ts"
      via: "executeWorkflow() and deleteWorkflow() for run and cleanup"
      pattern: "executeWorkflow|deleteWorkflow"
---

<objective>
Build the test harness that ties together deploy, execute, assert, and cleanup into a single `testWorkflow()` function with scenario-based assertions.

Purpose: This is the core feedback loop — Claude builds a workflow, tests it with sample data, and gets back pass/fail results with enough detail to diagnose and fix failures.

Output: `src/executor/test-harness.ts` and updated types in `src/executor/types.ts`.
</objective>

<execution_context>
@/Users/jurissleiners/.claude/get-shit-done/workflows/execute-plan.md
@/Users/jurissleiners/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/05.3-automated-workflow-testing/05.3-01-SUMMARY.md
@src/executor/types.ts
@src/executor/execute.ts
@src/deployer/deploy.ts
@src/deployer/types.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add test scenario and result types</name>
  <files>src/executor/types.ts</files>
  <action>
Add these types to the existing types.ts file (append, don't replace):

```typescript
/**
 * A test scenario: what to execute and what to expect.
 */
export interface TestScenario {
  name: string;                          // Scenario description (e.g., "valid user input")
  triggerData?: Record<string, unknown>; // Input data for manual trigger (pinned data) or webhook payload
  expectedStatus?: ExecutionStatus;      // Expected execution status (default: 'success')
  expectedNodes?: string[];              // Node names that must appear in execution data (order-insensitive)
  expectedOutput?: {                     // Assert on specific node output
    nodeName: string;                    // Which node to check
    assertions: OutputAssertion[];       // What to verify
  }[];
}

export interface OutputAssertion {
  field: string;                         // JSON path in output item (e.g., "result", "name")
  expected: unknown;                     // Expected value (deep equality)
}

/**
 * Result of a single test scenario execution.
 */
export interface TestResult {
  scenario: string;                      // Scenario name
  passed: boolean;                       // Overall pass/fail
  executionId?: string;                  // n8n execution ID
  duration: number;                      // Execution time in ms
  failures: TestFailure[];               // What went wrong (empty if passed)
  actualOutput?: NodeExecutionData[];    // Full execution output for diagnosis
  executionError?: ExecutionError;       // n8n execution error if any
}

export interface TestFailure {
  type: 'status' | 'missing_node' | 'output_mismatch' | 'execution_error' | 'timeout';
  message: string;
  expected?: unknown;
  actual?: unknown;
}

/**
 * Report for all scenarios in a test run.
 */
export interface TestReport {
  workflowName: string;
  workflowId?: string;                  // n8n workflow ID (for debugging)
  passed: number;
  failed: number;
  total: number;
  results: TestResult[];
  cleaned: boolean;                     // Whether workflow was deleted after testing
}
```

These types are the contract between the test harness and consuming code (Claude, vitest tests, etc.).
  </action>
  <verify>`npx tsc --noEmit` passes.</verify>
  <done>TestScenario, TestResult, TestReport, TestFailure, OutputAssertion types exported from src/executor/types.ts.</done>
</task>

<task type="auto">
  <name>Task 2: Build testWorkflow function</name>
  <files>src/executor/test-harness.ts</files>
  <action>
Create `src/executor/test-harness.ts` that exports a single `testWorkflow` function:

```typescript
export async function testWorkflow(
  builder: WorkflowBuilder,
  scenarios: TestScenario[],
  options?: ExecutionOptions
): Promise<TestReport>
```

Implementation flow:

1. **Deploy**: Call `deployWorkflow(builder, { apiUrl, apiKey, activate: true })` to create and activate the workflow. Catch deploy errors and return early with all scenarios failed.

2. **Execute each scenario**: For each TestScenario:
   a. If scenario has `triggerData`, pin the data to the Manual Trigger node before executing. Research in Plan 01 should reveal how to pass test data — either via `POST /api/v1/workflows/{id}/execute` with a body, or by setting pinned data on the workflow via PATCH before executing.
   b. Call `executeWorkflow(workflowId, options)` to trigger and poll.
   c. Record start time, execution ID, duration.

3. **Assert**: For each scenario, check:
   a. `expectedStatus`: Compare execution status. Default to 'success' if not specified.
   b. `expectedNodes`: Verify each named node appears in execution data.
   c. `expectedOutput`: For each output assertion, find the node in execution data and deep-compare the field value. Use `JSON.stringify` comparison for objects.
   d. Record all failures with descriptive messages including expected vs actual values.

4. **Build TestResult** for each scenario with:
   - `passed`: true only if zero failures
   - `failures`: array of TestFailure objects with type, message, expected, actual
   - `actualOutput`: full node execution data (so Claude can diagnose)
   - `executionError`: if execution failed, include the error object

5. **Cleanup**: Always delete the workflow (even if tests fail). Use try/catch so cleanup failure doesn't mask test results. Set `cleaned` boolean in report.

6. **Return TestReport** with aggregate counts and all results.

Key design decisions:
- The function ALWAYS cleans up (deletes workflow) — no leftover workflows in n8n
- Failures include actual output so Claude can self-diagnose
- The function works with Manual Trigger workflows (safest for testing, no external deps)
- Activate is always true (n8n needs active workflows to execute them)
- If a scenario execution times out, record it as a timeout failure, continue to next scenario

Import from:
- `deployWorkflow` from '../deployer/deploy.js'
- `executeWorkflow`, `deleteWorkflow` from './execute.js'
- Types from './types.js'
- `WorkflowBuilder` from '../builder/types.js'
  </action>
  <verify>`npx tsc --noEmit` passes. Manual review confirms deploy -> execute -> assert -> cleanup flow.</verify>
  <done>testWorkflow function exported from src/executor/test-harness.ts. Deploys, executes scenarios, asserts with detailed failure messages, always cleans up.</done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` — no type errors
2. testWorkflow imports and uses deployWorkflow, executeWorkflow, deleteWorkflow correctly
3. TestReport includes enough detail for Claude to diagnose failures (actual output, execution errors)
4. Cleanup always runs (workflow deleted) even on test failure
5. Existing 80 tests still pass: `npx vitest run`
</verification>

<success_criteria>
- TestScenario type supports: name, triggerData, expectedStatus, expectedNodes, expectedOutput
- testWorkflow(builder, scenarios) deploys, executes, asserts, cleans up
- Failures include type, message, expected, actual for each assertion
- TestReport shows passed/failed/total with full details
- Workflow always deleted after testing (cleaned: true)
</success_criteria>

<output>
After completion, create `.planning/phases/05.3-automated-workflow-testing/05.3-02-SUMMARY.md`
</output>
